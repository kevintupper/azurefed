{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Semantic kernel imports for the service, AOAI, and Ollama, and functions\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "from semantic_kernel.connectors.ai.prompt_execution_settings import PromptExecutionSettings\n",
    "\n",
    "\n",
    "# Local imports\n",
    "from services import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the environent\n",
    "load_dotenv()\n",
    "\n",
    "# Load the environment variables\n",
    "SK_AOAI_API_KEY = os.getenv(\"SK_AOAI_API_KEY\")\n",
    "SK_AOAI_ENDPOINT = os.getenv(\"SK_AOAI_ENDPOINT\")\n",
    "SK_AOAI_API_VERSION = os.getenv(\"SK_AOAI_API_VERSION\")\n",
    "SK_AOAI_DEPLOYMENT_NAME = os.getenv(\"SK_AOAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Setup the services\n",
    "kernel = Kernel()\n",
    "selectedService = Service.AzureOpenAI\n",
    "# selectedService = Service.Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make services available to the kernel\n",
    "\n",
    "# AOAI\n",
    "kernel.add_service(AzureChatCompletion(service_id=\"aoai\", deployment_name=SK_AOAI_DEPLOYMENT_NAME, endpoint=SK_AOAI_ENDPOINT, api_key=SK_AOAI_API_KEY), overwrite=True)\n",
    "\n",
    "# Ollama (phi3mini, mixtral8:22b)\n",
    "kernel.add_service(OllamaChatCompletion(service_id=\"phi3\", ai_model_id=\"phi3:mini\", url=\"http://localhost:11434/api/chat\"), overwrite=True)\n",
    "kernel.add_service(OllamaChatCompletion(service_id=\"mixtral\", ai_model_id=\"mixtral:8x22b\", url=\"http://localhost:11434/api/chat\"), overwrite=True)\n",
    "kernel.add_service(OllamaChatCompletion(service_id=\"llama3\", ai_model_id=\"llama3:70b\", url=\"http://localhost:11434/api/chat\"), overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the prompt execution settings and the plugin\n",
    "settings=PromptExecutionSettings(service_id=\"phi3\")\n",
    "\n",
    "plugin = kernel.add_plugin(parent_directory=\"./prompt_template_samples/\", plugin_name=\"FunPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the time-traveling paleontologist take a suitcase full of toothbrushes back to the dinosaur age?\n",
      "\n",
      "Because even though they had massive teeth, dinosaurs didn't have electric toothbrushes! And who wants to brush their teeth manually when you can bring an entire suitcase of modern ones?! It was super silly, but at least those giant lizards would have excellent oral hygiene!\n"
     ]
    }
   ],
   "source": [
    "joke_function = plugin[\"Joke\"]\n",
    "joke = await kernel.invoke(joke_function, KernelArguments(settings=PromptExecutionSettings(service_id=\"mixtral\"), input=\"time travel to dinosaur age\", style=\"super silly\"))\n",
    "\n",
    "\n",
    "print(joke)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
